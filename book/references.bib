
@article{song2022comprehensive,
  title={A Comprehensive Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities},
  author={Yisheng Song and Ting-Yuan Wang and Subrota Kumar Mondal and Jyoti Prakash Sahoo},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.06743}
}


% Metric-based few shot learning
@inproceedings{snell2017prototypical,
 author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Prototypical Networks for Few-shot Learning},
 volume = {30},
 year = {2017}
}

@inproceedings{vinyals2016matching,
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
title = {Matching Networks for One Shot Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3637–3645},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{sung2018relation,
author = {Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip H.S. and Hospedales, Timothy M.},
title = {Learning to Compare: Relation Network for Few-Shot Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@INPROCEEDINGS{wang-fewshotsed-2020,
  author={Y. {Wang} and J. {Salamon} and N. J. {Bryan} and J. {Pablo Bello}},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Few-Shot Sound Event Detection}, 
  year={2020},
  volume={},
  number={},
  pages={81-85},
  doi={10.1109/ICASSP40776.2020.9054708}
}

%optimization-based few-shot learning
@InProceedings{finn2017model,
  title = 	 {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author =       {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1126--1135},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/finn17a.html},
  abstract = 	 {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}
}


%memory-based few-shot learning
@inproceedings{santoro2016meta,
  author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  title = {Meta-Learning with Memory-Augmented Neural Networks},
  year = {2016},
  publisher = {JMLR.org},
  abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
  booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
  pages = {1842–1850},
  numpages = {9},
  location = {New York, NY, USA},
  series = {ICML'16}
}


% applications

@inproceedings{wang2020fewshotdrum,
      title={Few-Shot Drum Transcription in Polyphonic Music}, 
      author={Yu Wang and Justin Salamon and Mark Cartwright and Nicholas J. Bryan and Juan Pablo Bello},
      year={2020},
 booktitle = {International Society for Music Information Retrieval (ISMIR) Conference},
}

@inproceedings{flores2021leveraging,
  title = {Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition},
  author = {Flores Garcia, Hugo and Aguilar, Aldo and Manilow, Ethan and Pardo, Bryan},
  booktitle = {Proceedings of the 22nd International Society of Music Information Retrieval Conference (ISMIR 2021)},
  year = {2021},
  url = {https://interactiveaudiolab.github.io/assets/papers/flores2021leveraging.pdf}
}

@INPROCEEDINGS{wang2022fewshot,
  author={Wang, Yu and Stoller, Daniel and Bittner, Rachel M. and Pablo Bello, Juan},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Few-Shot Musical Source Separation}, 
  year={2022},
  volume={},
  number={},
  pages={121-125},
  doi={10.1109/ICASSP43922.2022.9747536}
}


% hierarchical few-shot
@article{hornbostel1961classification,
 ISSN = {00720127},
 author = {Erich M. von Hornbostel and Curt Sachs},
 journal = {The Galpin Society Journal},
 pages = {3--29},
 publisher = {Galpin Society},
 title = {Classification of Musical Instruments: Translated from the Original German by Anthony Baines and Klaus P. Wachsmann},
 volume = {14},
 year = {1961}
}


% datasets

@inproceedings{bittner-medleydb-2014,
  title={MedleyDB: A Multitrack Dataset for Annotation-Intensive
MIR Research},
  author={Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
  booktitle={15th International Society for Music Information Retrieval Conference (ISMIR)},  
  year={2014},
}



@dataset{bittner-medleydb2-2016,
  author       = {Rachel Bittner and
                  Julia Wilkins and
                  Hanna Yip and
                  Juan Pablo Bello},
  title        = {MedleyDB 2.0 Audio},
  month        = aug,
  year         = 2016,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.1715175},
  url          = {https://doi.org/10.5281/zenodo.1715175}
}

