{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Few-Shot Learning Episodes\n",
    "\n",
    "Last chapter, we learned how to create a class-conditional dataset for few-shot learning, and we implemented our own class-conditional version of the TinySOL dataset. In this chapter, we will build an Episodic Dataset using PyTorch, which will allow us to sample few-shot learning episodes from our dataset. \n",
    "\n",
    "This chapter, we will learn how to create an Episodic Dataset for a [Class Conditional Dataset](/fsl-example/datasets.html), and use it sample few-shot learning episodes. \n",
    "\n",
    "To recap on our [foundations chapter](/foundations-fsl/foundations.md), episodic training is a technique used in few-shot learning to effectively leverage a large training dataset. It involves splitting each training iteration into a self-contained learning task, known as an episode, which simulates a few-shot learning scenario with a small number of labeled examples for a set of classes. During episodic training, the model is presented with a completely new $N$-shot, $K$-way classification task at each step, and must learn to classify the examples in the query set using only the labeled examples in the support set. This allows the model to learn how to effectively learn from a small amount of data and adapt to new tasks quickly.\n",
    "\n",
    "## Anatomy of an Episode\n",
    "\n",
    "```{figure} ../assets/foundations/support-query.png\n",
    "---\n",
    "name: support-query\n",
    "---\n",
    "A few-shot learning episode splits data into two separate sets: the support set (the few labeled examples of novel data) and the query set (the data we want to label).\n",
    "```\n",
    "\n",
    "In few-shot learning, an episode consists of two sets of data: the support set and the query set.\n",
    "\n",
    "- The support set contains a small number of labeled examples for each of the classes in the episode. We use the examples in the support set to guide the few-shot learning model in the classification task. \n",
    "\n",
    "- The query set contains a larger number of (unlabeled) examples for each of the classes. During training, we make predictions for examples in the query set, and compute a loss over these predictions to update the model parameters. During evaluation, we use the predictions for the query set to compute any evaluation metrics for the episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements (hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch\n",
    "!pip install music-fsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from music_fsl.data import ClassConditionalDataset\n",
    "import music_fsl.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an `EpisodeDataset` class\n",
    "\n",
    "To sample few-shto learning episodes, we'll be writing a dataset class called `EpisodeDataset` that will act as a wrapper around a `ClassConditionalDataset`. \n",
    "\n",
    "Just like any other PyTorch dataset, we'll have to implement the `__len__` and `__getitem__` methods.\n",
    "Let's start by writing an `__init__` method, which will be responsible for initializing the episode dataset. \n",
    "\n",
    "We'll add in the ability to specify the number of classes to sample per episode (`n_way`), the number of support examples to sample per class (`n_support`), and the number of query examples to sample per class (`n_query`). We'll also add in the ability to specify the number of episodes to sample (`n_episodes`).\n",
    "\n",
    "```python\n",
    "class EpisodeDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        A dataset for sampling few-shot learning tasks from a class-conditional dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (ClassConditionalDataset): The dataset to sample episodes from.\n",
    "        n_way (int): The number of classes to sample per episode.\n",
    "            Default: 5.\n",
    "        n_support (int): The number of samples per class to use as support.\n",
    "            Default: 5.\n",
    "        n_query (int): The number of samples per class to use as query.\n",
    "            Default: 20.\n",
    "        n_episodes (int): The number of episodes to generate.\n",
    "            Default: 100.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dataset: ClassConditionalDataset, \n",
    "        n_way: int = 5, \n",
    "        n_support: int = 5,\n",
    "        n_query: int = 20,\n",
    "        n_episodes: int = 100,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "```\n",
    "\n",
    "## Sampling episodes: the `__getitem__` method\n",
    "Next, we will implement the `__getitem__` method. This method will be responsible for generating the actual episodes for training and evaluation. \n",
    "\n",
    "```python\n",
    "    def __getitem__(self, index: int) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Sample an episode from the class-conditional dataset. \n",
    "```\n",
    "\n",
    "We'll start by creating a random number generator based on the index of the episode we want to sample. This will allow us to sample the same episode every time we call `__getitem__` with the same index. \n",
    "\n",
    "```python\n",
    "        rng = random.Random(index)\n",
    "```\n",
    "\n",
    "First, we need to find out which subset of the classlist will be in the episode. We can do this by sampling `n_way` classes from the classlist. \n",
    "\n",
    "```python\n",
    "        # sample the list of classes for this episode\n",
    "        episode_classlist = rng.sample(self.dataset.classlist, self.n_way)\n",
    "```\n",
    "\n",
    "Next, we need to sample the support and query sets for each class. \n",
    "We can start creating empty lists for each set, and iterating through each of the classes:\n",
    "\n",
    "\n",
    "```python\n",
    "        # sample the support and query sets for this episode\n",
    "        support, query = [], []\n",
    "        for c in episode_classlist:\n",
    "```\n",
    "\n",
    "We need to sample `n_support` and `n_query` examples for each class. Because our dataset is an instance of a [Class Conditional Dataset](/fsl-example/datasets.html), we can use the `class_to_indices` attribute to get the indices of the examples for each class. \n",
    "\n",
    "```python\n",
    "            # grab the dataset indices for this class\n",
    "            all_indices = self.dataset.class_to_indices[c]\n",
    "```\n",
    "\n",
    "Once we have a hold of all the indices for that given class (`c`), we can grab `n_support + n_query` items from the dataset. \n",
    "\n",
    "```python\n",
    "            # sample the support and query sets for this class\n",
    "            indices = rng.sample(all_indices, self.n_support + self.n_query)\n",
    "            items = [self.dataset[i] for i in indices]\n",
    "```\n",
    "\n",
    "We can add the class target to each item we sampled. \n",
    "\n",
    "**NOTE**: note that the the index of the target is with respect to the `episode_classlist`. This is important, since we will use this index later to calculate the cross-entropy loss during training. \n",
    "\n",
    "```python\n",
    "            # add the class label to each item\n",
    "            for item in items:\n",
    "                item[\"target\"] = torch.tensor(episode_classlist.index(c))\n",
    "```\n",
    "\n",
    "Finally, we can split all the items we sampled into support and query items.\n",
    "```python\n",
    "            # split the support and query sets\n",
    "            support.extend(items[:self.n_support])\n",
    "            query.extend(items[self.n_support:]) \n",
    "```\n",
    "\n",
    "To wrap it up, we will collate the items in each set into a dictionary, to make batch processing possible. Since the details of writing a collating function aren't covered here, we invite the reader to check out the [`PyTorch Dataset docs`](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for more information.  \n",
    "```python\n",
    "        # collate the support and query sets\n",
    "        support = util.collate_list_of_dicts(support)\n",
    "        query = util.collate_list_of_dicts(query)\n",
    "\n",
    "        support[\"classlist\"] = episode_classlist\n",
    "        query[\"classlist\"] = episode_classlist\n",
    "        \n",
    "        return support, query\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unhide the cell below to see the full implementation of the `EpisodeDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "class EpisodeDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        A dataset for sampling few-shot learning tasks from a class-conditional dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (ClassConditionalDataset): The dataset to sample episodes from.\n",
    "        n_way (int): The number of classes to sample per episode.\n",
    "            Default: 5.\n",
    "        n_support (int): The number of samples per class to use as support.\n",
    "            Default: 5.\n",
    "        n_query (int): The number of samples per class to use as query.\n",
    "            Default: 20.\n",
    "        n_episodes (int): The number of episodes to generate.\n",
    "            Default: 100.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dataset: ClassConditionalDataset, \n",
    "        n_way: int = 5, \n",
    "        n_support: int = 5,\n",
    "        n_query: int = 20,\n",
    "        n_episodes: int = 100,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Sample an episode from the class-conditional dataset. \n",
    "\n",
    "        Each episode is a tuple of two dictionaries: a support set and a query set.\n",
    "        The support set contains a set of samples from each of the classes in the\n",
    "        episode, and the query set contains another set of samples from each of the\n",
    "        classes. The class labels are added to each item in the support and query\n",
    "        sets, and the list of classes is also included in each dictionary.\n",
    "\n",
    "        Yields:\n",
    "            Tuple[Dict[str, Any], Dict[str, Any]]: A tuple containing the support\n",
    "            set and the query set for an episode.\n",
    "        \"\"\"\n",
    "        # seed the random number generator so we can reproduce this episode\n",
    "        rng = random.Random(index)\n",
    "\n",
    "        # sample the list of classes for this episode\n",
    "        episode_classlist = rng.sample(self.dataset.classlist, self.n_way)\n",
    "\n",
    "        # sample the support and query sets for this episode\n",
    "        support, query = [], []\n",
    "        for c in episode_classlist:\n",
    "            # grab the dataset indices for this class\n",
    "            all_indices = self.dataset.class_to_indices[c]\n",
    "\n",
    "            # sample the support and query sets for this class\n",
    "            indices = rng.sample(all_indices, self.n_support + self.n_query)\n",
    "            items = [self.dataset[i] for i in indices]\n",
    "\n",
    "            # add the class label to each item\n",
    "            for item in items:\n",
    "                item[\"target\"] = torch.tensor(episode_classlist.index(c))\n",
    "\n",
    "            # split the support and query sets\n",
    "            support.extend(items[:self.n_support])\n",
    "            query.extend(items[self.n_support:])\n",
    "\n",
    "        # collate the support and query sets\n",
    "        support = util.collate_list_of_dicts(support)\n",
    "        query = util.collate_list_of_dicts(query)\n",
    "\n",
    "        support[\"classlist\"] = episode_classlist\n",
    "        query[\"classlist\"] = episode_classlist\n",
    "        \n",
    "        return support, query\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "\n",
    "    def print_episode(self, support, query):\n",
    "        \"\"\"Print a summary of the support and query sets for an episode.\n",
    "\n",
    "        Args:\n",
    "            support (Dict[str, Any]): The support set for an episode.\n",
    "            query (Dict[str, Any]): The query set for an episode.\n",
    "        \"\"\"\n",
    "        print(\"Support Set:\")\n",
    "        print(f\"  Classlist: {support['classlist']}\")\n",
    "        print(f\"  Audio Shape: {support['audio'].shape}\")\n",
    "        print(f\"  Target Shape: {support['target'].shape}\")\n",
    "        print()\n",
    "        print(\"Query Set:\")\n",
    "        print(f\"  Classlist: {query['classlist']}\")\n",
    "        print(f\"  Audio Shape: {query['audio'].shape}\")\n",
    "        print(f\"  Target Shape: {query['target'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together: Sampling an Example Episode\n",
    "\n",
    "Super! Let's grab the class-conditional `TinySol` we created last chapter, and use the `EpisodeDataset` to sample an episode from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from music_fsl.data import TinySOL\n",
    "\n",
    "dataset = TinySOL()\n",
    "\n",
    "# create an episodic dataset\n",
    "episodes = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_way=5, \n",
    "    n_support=5,\n",
    "    n_query=20,\n",
    "    n_episodes=100,\n",
    ")\n",
    "\n",
    "support, query = episodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Set:\n",
      "  Classlist: ['Accordion', 'Violin', 'Oboe', 'Bassoon', 'Alto Saxophone']\n",
      "  Audio Shape: torch.Size([25, 1, 16000])\n",
      "  Target Shape: torch.Size([25])\n",
      "\n",
      "Query Set:\n",
      "  Classlist: ['Accordion', 'Violin', 'Oboe', 'Bassoon', 'Alto Saxophone']\n",
      "  Audio Shape: torch.Size([100, 1, 16000])\n",
      "  Target Shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "episodes.print_episode(support, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we learned how to create an EpisodicSampler class that extends the Sampler class in PyTorch to sample few-shot learning episodes. The EpisodicSampler allows us to specify the number of classes to sample per episode, the number of support and query examples to sample per class, and the number of episodes to sample. It iterates over the episodes and yields a support and query set for each episode, where the support set contains labeled examples for each of the classes in the episode and the query set contains unlabeled examples for each of the classes. This allows us to use the EpisodicSampler to generate few-shot learning tasks for training and evaluation.\n",
    "\n",
    "Next, we'll write code to create a Prototypical Network model that can be trained on the episodes generated by the EpisodicSampler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('hugo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec27501cbbac6acacfd09d8db1502718360d0cdb5a917685adcae650b3d3518d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
