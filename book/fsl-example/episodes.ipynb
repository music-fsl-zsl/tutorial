{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Few-Shot Learning Episodes\n",
    "\n",
    "Last chapter, we learned how to create a class-conditional dataset for few-shot learning, and we implemented our own class-conditional version of the TinySOL dataset. In this chapter, we will build an Episodic Sampler using PyTorch, which will allow us to sample few-shot learning episodes from our dataset. \n",
    "\n",
    "This chapter, we will learn how to create an Episodic Sampler for a [Class Conditional Dataset](/fsl-example/datasets.html), and use it sample few-shot learning episodes. \n",
    "\n",
    "To recap on our [foundations chapter](/foundations-fsl/foundations.md), episodic training is a technique used in few-shot learning to effectively leverage a large training dataset. It involves splitting each training iteration into a self-contained learning task, known as an episode, which simulates a few-shot learning scenario with a small number of labeled examples for a set of classes. During episodic training, the model is presented with a completely new $N$-shot, $K$-way classification task at each step, and must learn to classify the examples in the query set using only the labeled examples in the support set. This allows the model to learn how to effectively learn from a small amount of data and adapt to new tasks quickly.\n",
    "\n",
    "## Anatomy of an Episode\n",
    "\n",
    "```{figure} ../assets/foundations/support-query.png\n",
    "---\n",
    "name: support-query\n",
    "---\n",
    "A few-shot learning episode splits data into two separate sets: the support set (the few labeled examples of novel data) and the query set (the data we want to label).\n",
    "```\n",
    "\n",
    "In few-shot learning, an episode consists of two sets of data: the support set and the query set.\n",
    "\n",
    "- The support set contains a small number of labeled examples for each of the classes in the episode. We use the examples in the support set to guide the few-shot learning model in the classification task. \n",
    "\n",
    "- The query set contains a larger number of (unlabeled) examples for each of the classes. During training, we make predictions for examples in the query set, and compute a loss over these predictions to update the model parameters. During evaluation, we use the predictions for the query set to compute any evaluation metrics for the episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from music_fsl.data import ClassConditionalDataset\n",
    "import music_fsl.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an `EpisodicSampler` class\n",
    "\n",
    "To sample few-shot learning episodes, we can take advantage of the [`Sampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler) class in PyTorch. The `Sampler` class in PyTorch provides a convenient way to sample a subset of data from a dataset. It is an abstract class that can be extended to create custom sampling strategies for a dataset. In this section, we will implement an EpisodicSampler class that extends the Sampler class to generate few-shot learning episodes.\n",
    "\n",
    "To create our own `EpisodicSampler`, we will need to implement the following methods:\n",
    "\n",
    "- `__iter__`: The `__iter__` method is responsible for generating the episodes that we will use for training and evaluation. It should iterate over the episodes and yield an episode with a support and query set at each iteration.\n",
    "\n",
    "- `__len__`: The `__len__` method is responsible for returning the total number of episodes that the sampler will generate.\n",
    "\n",
    "\n",
    "Let's start by writing an `__init__` method, which will be responsible for initializing the sampler. \n",
    "\n",
    "We'll add in the ability to specify the number of classes to sample per episode (`n_way`), the number of support examples to sample per class (`n_support`), and the number of query examples to sample per class (`n_query`). We'll also add in the ability to specify the number of episodes to sample (`n_episodes`).\n",
    "\n",
    "```python\n",
    "class EpisodeSampler(torch.utils.data.Sampler):\n",
    "   \"\"\"\n",
    "    A sampler for few-shot learning tasks.\n",
    "\n",
    "    Args:\n",
    "        dataset (ClassConditionalDataset): The dataset to sample episodes from.\n",
    "        n_way (int): The number of classes to sample per episode.\n",
    "            Default: 5.\n",
    "        n_support (int): The number of samples per class to use as support.\n",
    "            Default: 5.\n",
    "        n_query (int): The number of samples per class to use as query.\n",
    "            Default: 20.\n",
    "        n_episodes (int): The number of episodes to generate.\n",
    "            Default: 100.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dataset: ClassConditionalDataset, \n",
    "        n_way: int = 5, \n",
    "        n_support: int = 5,\n",
    "        n_query: int = 20,\n",
    "        n_episodes: int = 100,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "```\n",
    "\n",
    "## Sampling episodes: the `__iter__` method\n",
    "Next, we will implement the `__iter__` method. This method will be responsible for generating the actual episodes for training and evaluation. \n",
    "\n",
    "```python\n",
    "def __iter__(self):\n",
    "    \"\"\"Sample an episode from the class conditional dataset\"\"\"\n",
    "```\n",
    "\n",
    "First, we need to find out which subset of the classlist will be in the episode. We can do this by sampling `n_way` classes from the classlist. \n",
    "\n",
    "```python\n",
    "        # sample the list of classes for this episode\n",
    "        episode_classlist = random.sample(self.dataset.classlist, self.n_way)\n",
    "```\n",
    "\n",
    "Next, we need to sample the support and query sets for each class. \n",
    "We can start creating empty lists for each set, and iterating through each of the classes:\n",
    "\n",
    "\n",
    "```python\n",
    "        # sample the support and query sets for this episode\n",
    "        support, query = [], []\n",
    "        for c in episode_classlist:\n",
    "```\n",
    "\n",
    "We need to sample `n_support` and `n_query` examples for each class. Because our dataset is an instance of a [Class Conditional Dataset](/fsl-example/datasets.html), we can use the `class_to_indices` attribute to get the indices of the examples for each class. \n",
    "\n",
    "```python\n",
    "            # grab the dataset indices for this class\n",
    "            all_indices = self.dataset.class_to_indices[c]\n",
    "```\n",
    "\n",
    "Once we have a hold of all the indices for that given class (`c`), we can grab `n_support + n_query` items from the dataset. \n",
    "\n",
    "```python\n",
    "            # sample the support and query sets for this class\n",
    "            indices = random.sample(all_indices, self.n_support + self.n_query)\n",
    "            items = [self.dataset[i] for i in indices]\n",
    "```\n",
    "\n",
    "We can add the class target to each item we sampled. \n",
    "\n",
    "**NOTE**: note that the the index of the target is with respect to the `episode_classlist`. This is important, since we will use this index later to calculate the cross-entropy loss during training. \n",
    "\n",
    "```python\n",
    "            # add the class label to each item\n",
    "            for item in items:\n",
    "                item[\"target\"] = torch.tensor(episode_classlist.index(c))\n",
    "```\n",
    "\n",
    "Finally, we can split all the items we sampled into support and query items.\n",
    "```python\n",
    "            # split the support and query sets\n",
    "            support.extend(items[:self.n_support])\n",
    "            query.extend(items[self.n_support:]) \n",
    "```\n",
    "\n",
    "To wrap it up, we will collate the items in each set into a dictionary, to make batch processing possible. Since the details of writing a collating function aren't covered here, we invite the reader to check out the [`PyTorch Dataset docs`](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for more information.  \n",
    "```python\n",
    "        # collate the support and query sets\n",
    "        support = util.collate_list_of_dicts(support)\n",
    "        query = util.collate_list_of_dicts(query)\n",
    "\n",
    "        support[\"classlist\"] = episode_classlist\n",
    "        query[\"classlist\"] = episode_classlist\n",
    "        \n",
    "        yield support, query\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unhide the cell below to see the full implementation of the `EpisodeSampler` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class EpisodeSampler(torch.utils.data.Sampler):\n",
    "    \"\"\"\n",
    "        A sampler for few-shot learning tasks.\n",
    "\n",
    "    Args:\n",
    "        dataset (ClassConditionalDataset): The dataset to sample episodes from.\n",
    "        n_way (int): The number of classes to sample per episode.\n",
    "            Default: 5.\n",
    "        n_support (int): The number of samples per class to use as support.\n",
    "            Default: 5.\n",
    "        n_query (int): The number of samples per class to use as query.\n",
    "            Default: 20.\n",
    "        n_episodes (int): The number of episodes to generate.\n",
    "            Default: 100.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dataset: ClassConditionalDataset, \n",
    "        n_way: int = 5, \n",
    "        n_support: int = 5,\n",
    "        n_query: int = 20,\n",
    "        n_episodes: int = 100,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        self.n_episodes = n_episodes\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate through the episodes generated by the sampler. \n",
    "\n",
    "        Each episode is a tuple of two dictionaries: a support set and a query set.\n",
    "        The support set contains a set of samples from each of the classes in the\n",
    "        episode, and the query set contains another set of samples from each of the\n",
    "        classes. The class labels are added to each item in the support and query\n",
    "        sets, and the list of classes is also included in each dictionary.\n",
    "\n",
    "        Yields:\n",
    "            Tuple[Dict[str, Any], Dict[str, Any]]: A tuple containing the support\n",
    "            set and the query set for an episode.\n",
    "        \"\"\"\n",
    "        # sample the list of classes for this episode\n",
    "        episode_classlist = random.sample(self.dataset.classlist, self.n_way)\n",
    "\n",
    "        # sample the support and query sets for this episode\n",
    "        support, query = [], []\n",
    "        for c in episode_classlist:\n",
    "            # grab the dataset indices for this class\n",
    "            all_indices = self.dataset.class_to_indices[c]\n",
    "\n",
    "            # sample the support and query sets for this class\n",
    "            indices = random.sample(all_indices, self.n_support + self.n_query)\n",
    "            items = [self.dataset[i] for i in indices]\n",
    "\n",
    "            # add the class label to each item\n",
    "            for item in items:\n",
    "                item[\"target\"] = torch.tensor(episode_classlist.index(c))\n",
    "\n",
    "            # split the support and query sets\n",
    "            support.extend(items[:self.n_support])\n",
    "            query.extend(items[self.n_support:])\n",
    "\n",
    "        # collate the support and query sets\n",
    "        support = util.collate_list_of_dicts(support)\n",
    "        query = util.collate_list_of_dicts(query)\n",
    "\n",
    "        support[\"classlist\"] = episode_classlist\n",
    "        query[\"classlist\"] = episode_classlist\n",
    "        \n",
    "        yield support, query\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "\n",
    "    def print_episode(self, support, query):\n",
    "        \"\"\"Print a summary of the support and query sets for an episode.\n",
    "\n",
    "        Args:\n",
    "            support (Dict[str, Any]): The support set for an episode.\n",
    "            query (Dict[str, Any]): The query set for an episode.\n",
    "        \"\"\"\n",
    "        print(\"Support Set:\")\n",
    "        print(f\"  Classlist: {support['classlist']}\")\n",
    "        print(f\"  Audio Shape: {support['audio'].shape}\")\n",
    "        print(f\"  Target Shape: {support['target'].shape}\")\n",
    "        print()\n",
    "        print(\"Query Set:\")\n",
    "        print(f\"  Classlist: {query['classlist']}\")\n",
    "        print(f\"  Audio Shape: {query['audio'].shape}\")\n",
    "        print(f\"  Target Shape: {query['target'].shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together: Sampling an Example Episode\n",
    "\n",
    "Super! Let's grab the class-conditional `TinySol` we created last chapter, and use the `EpisodeSampler` to sample an episode from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [annotations] downloading TinySOL_metadata.csv\n",
      "INFO: /home/hugo/mir_datasets/tinysol/annotation/TinySOL_metadata.csv already exists and will not be downloaded. Rerun with force_overwrite=True to delete this file and force the download.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from music_fsl.data import TinySOL\n",
    "\n",
    "dataset = TinySOL()\n",
    "\n",
    "# create an episodic dataset\n",
    "sampler = EpisodeSampler(\n",
    "    dataset,\n",
    "    n_way=5, \n",
    "    n_support=5,\n",
    "    n_query=20,\n",
    "    n_episodes=100,\n",
    ")\n",
    "\n",
    "support, query = next(iter(sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Set:\n",
      "  Classlist: ['Oboe', 'Bassoon', 'Viola', 'Trombone', 'French Horn']\n",
      "  Audio Shape: torch.Size([25, 1, 16000])\n",
      "  Target Shape: torch.Size([25])\n",
      "\n",
      "Query Set:\n",
      "  Classlist: ['Oboe', 'Bassoon', 'Viola', 'Trombone', 'French Horn']\n",
      "  Audio Shape: torch.Size([100, 1, 16000])\n",
      "  Target Shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "sampler.print_episode(support, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we learned how to create an EpisodicSampler class that extends the Sampler class in PyTorch to sample few-shot learning episodes. The EpisodicSampler allows us to specify the number of classes to sample per episode, the number of support and query examples to sample per class, and the number of episodes to sample. It iterates over the episodes and yields a support and query set for each episode, where the support set contains labeled examples for each of the classes in the episode and the query set contains unlabeled examples for each of the classes. This allows us to use the EpisodicSampler to generate few-shot learning tasks for training and evaluation.\n",
    "\n",
    "Next, we'll write code to create a Prototypical Network model that can be trained on the episodes generated by the EpisodicSampler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('hugo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec27501cbbac6acacfd09d8db1502718360d0cdb5a917685adcae650b3d3518d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
