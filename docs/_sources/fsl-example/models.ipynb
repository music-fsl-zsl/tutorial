{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Prototypical Network\n",
    "\n",
    "In this part of the tutorial, we'll be building the prediction routine for a [prototypical network](/foundations-fsl/approaches.md) in PyTorch. \n",
    "\n",
    "\n",
    "To recap, prototypical networks are a type of metric-based few-shot learning method. They require a backbone model, which is used to compute the embeddings of the examples in the support and query sets. The prototypical network then creates a single prototype for each class in the support set, which is the mean of the embeddings of all the examples in the support set for that class. To classify a query example, the model compares it to each prototype using the squared Euclidean distance and applies a softmax function to the negated distances to obtain a probability distribution over the classes. The query example is then assigned to the class with the highest probability. This enables the model to learn to classify new classes using only a few examples per class.\n",
    "\n",
    "\n",
    "```{figure} ../assets/foundations/prototypical-net.png\n",
    "---\n",
    "name: protonet\n",
    "---\n",
    "A prototypical network. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements (hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch\n",
    "!pip install --no-cache-dir --upgrade music-fsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Backbone Model \n",
    "\n",
    "\n",
    "The first step in making our prototypical network is making a backbone model that is able to create embeddings from audio examples. \n",
    "\n",
    "For the sake of simplicity, we'll be using a fully convolutional model that takes in audio, computes a mel-spectrogram, and then applies a series of convolutional blocks to the spectrograms until it produces a `512`-dimensional embedding. We won't go into much detail about the architecture, since the architecture is not the focus of this tutorial. \n",
    "\n",
    "Unhide the code cell below to see the implementation of the backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MelSpectrogram\n",
    "from torch import nn\n",
    "import torch \n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional block, consisting of a convolution, group normalization,\n",
    "    ReLU activation, and max pooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        in_channels, out_channels, \n",
    "        kernel_size, stride, padding, \n",
    "        num_groups, max_pool_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.gn = nn.GroupNorm(num_groups, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(max_pool_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.gn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully convolutional model that produces 512-dimensional embeddings from audio samples. \n",
    "    \n",
    "    Args:\n",
    "        sample_rate (int): The sample rate of the input audio.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate: int):\n",
    "        super().__init__()\n",
    "        self.melspec = MelSpectrogram(\n",
    "            n_mels=64, sample_rate=sample_rate\n",
    "        )\n",
    "        \n",
    "        self.conv1 = ConvBlock(1, 32, 3, 1, 'same', 8, 2)\n",
    "        self.conv2 = ConvBlock(32, 64, 3, 1, 'same',16, 2)\n",
    "        self.conv3 = ConvBlock(64, 128, 3, 1, 'same', 32, 2)\n",
    "        self.conv4 = ConvBlock(128, 256, 3, 1, 'same', 64, 2)\n",
    "        self.conv5 = ConvBlock(256, 512, 1, 1, 'same', 128, 4)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.ndim == 3, \"Expected a batch of audio samples shape (batch, channels, samples)\"\n",
    "        assert x.shape[1] == 1, \"Expected a mono audio signal\"\n",
    "\n",
    "        x = self.melspec(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        # pool over the time dimension\n",
    "        # squeeze the (t, f) dimensions\n",
    "        x = x.mean(dim=-1)\n",
    "        x = x.squeeze(-1).squeeze(-1) # (batch, 512)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone(\n",
      "  (melspec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (conv1): ConvBlock(\n",
      "    (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (gn): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "    (relu): ReLU()\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): ConvBlock(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (gn): GroupNorm(16, 64, eps=1e-05, affine=True)\n",
      "    (relu): ReLU()\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): ConvBlock(\n",
      "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (gn): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "    (relu): ReLU()\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): ConvBlock(\n",
      "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (gn): GroupNorm(64, 256, eps=1e-05, affine=True)\n",
      "    (relu): ReLU()\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv5): ConvBlock(\n",
      "    (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "    (gn): GroupNorm(128, 512, eps=1e-05, affine=True)\n",
      "    (relu): ReLU()\n",
      "    (maxpool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "backbone = Backbone(sample_rate)\n",
    "\n",
    "print(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Prototypical Network\n",
    "\n",
    "Now for the fun part! It's time to write the prototypical network itself. \n",
    "\n",
    "Let's start by defining a new `nn.Module`. The backbone model is passed in as an argument to the constructor. We'll be using this backbone model to compute the embeddings of the examples in the support and query sets.\n",
    "\n",
    "```python\n",
    "\n",
    "class PrototypicalNet(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "```\n",
    "\n",
    "Now, we need to define the `forward` function. \n",
    "\n",
    "### The `forward` function\n",
    "\n",
    "The `forward` function takes in the support and query sets, which are going to be supplied by the [`EpisodeDataset`](/fsl-example/episodes.html) that we defined in the previous chapter. \n",
    "\n",
    "The `forward` function will carry out the following steps:\n",
    "1. Compute the embeddings of the examples in the support and query sets using the backbone model.\n",
    "2. Compute the prototypes for each class in the support set.\n",
    "3. Compute the distances between the query examples and the prototypes.\n",
    "4. Return the logits for the query examples (the negated distances).\n",
    "\n",
    "The support and query sets are going to be dictionaries with the following keys: \n",
    "```python\n",
    "            support (dict): A dictionary containing the support set. \n",
    "                The support set dict must contain the following keys:\n",
    "                    - audio: A tensor of shape (n_support, n_channels, n_samples)\n",
    "                    - label: A tensor of shape (n_support) with label indices\n",
    "                    - classlist: A tensor of shape (n_classes) containing the list of classes in this episode\n",
    "            query (dict): A dictionary containing the query set.\n",
    "                The query set dict must contain the following keys:\n",
    "                    - audio: A tensor of shape (n_query, n_channels, n_samples)\n",
    "```\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "```python\n",
    "    def forward(self, support: dict, query: dict):\n",
    "        \"\"\"\n",
    "        Forward pass through the protonet. \n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "#### 1. Computing the embeddings\n",
    "\n",
    "The first step is to compute the embeddings of the examples in the support and query sets. We'll do this by passing the audio tensors in the support and query sets to the backbone model. We'll update the support and query dictionaries in place to include the embeddings.\n",
    "\n",
    "```python\n",
    "        # compute the embeddings for the support and query sets\n",
    "        support[\"embeddings\"] = self.backbone(support[\"audio\"])\n",
    "        query[\"embeddings\"] = self.backbone(query[\"audio\"])\n",
    "```\n",
    "\n",
    "#### 2. Computing the prototypes\n",
    "\n",
    "Computing the prototypes is a little involved, since first we need to group the support embeddings by class.\n",
    "\n",
    "We'll iterate through the indices in the classlist, and grab the subset of embeddings whose target belongs to the current index in the classlist. \n",
    "```python\n",
    "        # group the support embeddings by class\n",
    "        support_embeddings = []\n",
    "        for idx in range(len(support[\"classlist\"])):\n",
    "            # only keep the subset of embeddings whose target is the current index in the classlist\n",
    "            embeddings = support[\"embeddings\"][support[\"target\"] == idx]\n",
    "            support_embeddings.append(embeddings)\n",
    "        support_embeddings = torch.stack(support_embeddings)\n",
    "```\n",
    "\n",
    "After grouping them, the `support_embeddings` tensor will have shape `(n_classes, n_support, embedding_dim)`.\n",
    "\n",
    "Now, we can compute the prototypes by taking the mean of the embeddings for each class. We'll append the prototypes to the support dictionary in place.\n",
    "\n",
    "```python\n",
    "        # compute the prototypes for each class\n",
    "        prototypes = support_embeddings.mean(dim=1)\n",
    "        support[\"prototypes\"] = prototypes\n",
    "```\n",
    "\n",
    "The `prototypes` tensor will have shape `(n_classes, embedding_dim)`.\n",
    "\n",
    "#### 3. Computing the distances\n",
    "\n",
    "The next step in making our prototypical network is computing the distances between the query examples and the prototypes.\n",
    "\n",
    "Luckily, torch has a very nifty function for computing the pairwise distances between two tensors of shape `(batch, n, d)` and `(batch, m, d)`, which is [`torch.cdist`](https://pytorch.org/docs/stable/generated/torch.cdist.html).\n",
    "\n",
    "Because we're leaving out the batch dimension in this example, we'll unsqueeze a batch dimension to the query embeddings and prototypes tensors.\n",
    "\n",
    "Remember that prototypical networks use the squared Euclidean distance, so we'll set the `p` norm to `2` and square the result. \n",
    "\n",
    "```python\n",
    "        # compute the distances between each query and prototype\n",
    "        distances = torch.cdist(\n",
    "            query[\"embeddings\"].unsqueeze(0), \n",
    "            prototypes.unsqueeze(0),\n",
    "            p=2\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # square the distances to get the sq euclidean distance\n",
    "        distances = distances ** 2\n",
    "```\n",
    "\n",
    "#### 4. Computing the logits\n",
    "\n",
    "Finally, we need to negate the distances so that they can be used as log-probabilities (logits). \n",
    "\n",
    "```python\n",
    "        # negate the distances to get the logits\n",
    "        logits = -distances\n",
    "        return logits\n",
    "```\n",
    "\n",
    "Unhide the code cell below to see the full implementation of the prototypical network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class PrototypicalNet(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "    \n",
    "    def forward(self, support: dict, query: dict):\n",
    "        \"\"\"\n",
    "        Forward pass through the protonet. \n",
    "\n",
    "        Args:\n",
    "            support (dict): A dictionary containing the support set. \n",
    "                The support set dict must contain the following keys:\n",
    "                    - audio: A tensor of shape (n_support, n_channels, n_samples)\n",
    "                    - label: A tensor of shape (n_support) with label indices\n",
    "                    - classlist: A tensor of shape (n_classes) containing the list of classes in this episode\n",
    "            query (dict): A dictionary containing the query set.\n",
    "                The query set dict must contain the following keys:\n",
    "                    - audio: A tensor of shape (n_query, n_channels, n_samples)\n",
    "        \n",
    "        Returns:\n",
    "            logits (torch.Tensor): A tensor of shape (n_query, n_classes) containing the logits\n",
    "\n",
    "        After the forward pass, the support dict is updated with the following keys:\n",
    "            - embeddings: A tensor of shape (n_support, n_features) containing the embeddings\n",
    "            - prototypes: A tensor of shape (n_classes, n_features) containing the prototypes\n",
    "        \n",
    "        The query dict is updated with\n",
    "            - embeddings: A tensor of shape (n_query, n_features) containing the embeddings\n",
    "\n",
    "        \"\"\"\n",
    "        # compute the embeddings for the support and query sets\n",
    "        support[\"embeddings\"] = self.backbone(support[\"audio\"])\n",
    "        query[\"embeddings\"] = self.backbone(query[\"audio\"])\n",
    "\n",
    "        # group the support embeddings by class\n",
    "        support_embeddings = []\n",
    "        for idx in range(len(support[\"classlist\"])):\n",
    "            embeddings = support[\"embeddings\"][support[\"target\"] == idx]\n",
    "            support_embeddings.append(embeddings)\n",
    "        support_embeddings = torch.stack(support_embeddings)\n",
    "\n",
    "        # compute the prototypes for each class\n",
    "        prototypes = support_embeddings.mean(dim=1)\n",
    "        support[\"prototypes\"] = prototypes\n",
    "\n",
    "        # compute the distances between each query and prototype\n",
    "        distances = torch.cdist(\n",
    "            query[\"embeddings\"].unsqueeze(0), \n",
    "            prototypes.unsqueeze(0),\n",
    "            p=2\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # square the distances to get the sq euclidean distance\n",
    "        distances = distances ** 2\n",
    "        logits = -distances\n",
    "\n",
    "        # return the logits\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's put all of the pieces we've created so far (the class-conditional dataset, the episodic sampler, and the prototypical network) together to get some logits for a training episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from music_fsl.data import TinySOL, EpisodeDataset\n",
    "\n",
    "sample_rate = 16000\n",
    "\n",
    "# create a class-conditional dataset\n",
    "dataset = TinySOL(sample_rate=sample_rate)\n",
    "\n",
    "# create an episodic sampler\n",
    "episodes = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_way=5, \n",
    "    n_support=5,\n",
    "    n_query=20,\n",
    "    n_episodes=100,\n",
    ")\n",
    "\n",
    "support, query = episodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Set:\n",
      "  Classlist: ['Accordion', 'Violin', 'Oboe', 'Bassoon', 'Alto Saxophone']\n",
      "  Audio Shape: torch.Size([25, 1, 16000])\n",
      "  Target Shape: torch.Size([25])\n",
      "\n",
      "Query Set:\n",
      "  Classlist: ['Accordion', 'Violin', 'Oboe', 'Bassoon', 'Alto Saxophone']\n",
      "  Audio Shape: torch.Size([100, 1, 16000])\n",
      "  Target Shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "episodes.print_episode(support, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got logits with shape torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "# create our backbone model\n",
    "backbone = Backbone(sample_rate)\n",
    "\n",
    "# create a prototypical net\n",
    "protonet = PrototypicalNet(backbone)\n",
    "\n",
    "# compute the logits for the sample episode\n",
    "logits = protonet(support, query)\n",
    "print(f\"got logits with shape {logits.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec27501cbbac6acacfd09d8db1502718360d0cdb5a917685adcae650b3d3518d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
