
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Zero-shot learning task formulation &#8212; Few-shot and Zero-shot Learning for Music Information Retrieval</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Side Information" href="03_side_information.html" />
    <link rel="prev" title="Zero-Shot Learning Foundations" href="01_overview_zsl.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5R4YF40M3R`"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-5R4YF40M3R`');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Few-shot and Zero-shot Learning for Music Information Retrieval</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../landing.html">
                    Few-Shot and Zero-Shot Learning for Music Information Retrieval
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro/fsl-and-zsl.html">
   What is Few-Shot Learning (FSL) and Zero-Shot Learning (ZSL)?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intro/advantages.html">
   Advantages of FSL and ZSL in MIR
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundations: Few-Shot Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../foundations-fsl/foundations.html">
   Few-Shot Learning Foundations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../foundations-fsl/approaches.html">
   Approaches
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../foundations-fsl/metric-based-fsl.html">
   Metric-Based Few-Shot Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../foundations-fsl/optimization-based-fsl.html">
   Optimization-Based Few-Shot Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundations: Zero-Shot Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_overview_zsl.html">
   Zero-Shot Learning Foundations
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Zero-shot learning task formulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_side_information.html">
   Side Information
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Few-Shot Learning in PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../fsl-example/intro.html">
   Introduction: Few-Shot Learning in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fsl-example/datasets.html">
   Building a Class-Conditional Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fsl-example/episodes.html">
   Sampling Few-Shot Learning Episodes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fsl-example/models.html">
   Building a Prototypical Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fsl-example/training.html">
   Training a Few-Shot Instrument Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fsl-example/evaluating.html">
   Evaluating (and Visualizing) a Trained Prototypical Net
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Zero-Shot Learning in PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zsl-example/coding_example_zsl.html">
   Introduction: Zero-Shot Learning in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../zsl-example/data_prep.html">
   Prepare dataset and splits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../zsl-example/model.html">
   Models and data I/O
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../zsl-example/zsl_training_word_audio.html">
   Word-audio ZSL training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../zsl-example/zsl_eval_word_audio.html">
   Word-audio ZSL evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../zsl-example/zsl_training_image_audio.html">
   Image-audio ZSL training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../zsl-example/zsl_eval_image_audio.html">
   Image-audio ZSL evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Recent Advances
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../advances/introduction.html">
   Recent Advances in MIR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../advances/classification.html">
   Classification: Musical Instrument Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../advances/classification_sed.html">
   Classification: Sound Event Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../advances/classification_zsl.html">
   Classification: Music Classification and Tagging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../advances/transcription.html">
   Drum Transcription
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../advances/source-sep.html">
   Music Source Separation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Conclusions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/future_dir.html">
   Future Directions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/summary.html">
   Summary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bibliography.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/music-fsl-zsl/tutorial"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/music-fsl-zsl/tutorial/issues/new?title=Issue%20on%20page%20%2Ffoundations-zsl/02_task_formulation.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/foundations-zsl/02_task_formulation.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Zero-shot learning task formulation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problem-definition">
     Problem definition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modeling-f">
     Modeling
     <span class="math notranslate nohighlight">
      \(f\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#compatibility-between-the-input-data-and-the-label-data">
       Compatibility between the input data and the label data.
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#maximizing-the-compatibility">
         1-1) Maximizing the compatibility.
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#minimizing-a-distance-loss-function">
         1-2) Minimizing a distance loss function.
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#synthesizing-zero-shot-class-embeddings-given-phi-y">
     Synthesizing zero-shot class embeddings given
     <span class="math notranslate nohighlight">
      \(\phi(y)\)
     </span>
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#available-data-while-training">
     Available data while training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zero-shot-evlauation-scheme">
   Zero-shot evlauation scheme
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-zero-shot-evaluation-setup">
     ‘Generalized’ zero-shot evaluation setup
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#different-approaches-for-zero-shot-learning">
   Different approaches for zero-shot learning
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-1-learning-by-pairwise-ranking-of-compatibility">
     (1) Case 1 : Learning by pairwise ranking of compatibility
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-2-learning-by-maximizing-probability-function">
     (2) Case 2 : Learning by maximizing probability function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-3-autoencoder-approach">
     (3) Case 3 : Autoencoder approach
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-4-generative-approach">
     (4) Case 4 : Generative approach
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Zero-shot learning task formulation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Zero-shot learning task formulation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problem-definition">
     Problem definition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modeling-f">
     Modeling
     <span class="math notranslate nohighlight">
      \(f\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#compatibility-between-the-input-data-and-the-label-data">
       Compatibility between the input data and the label data.
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#maximizing-the-compatibility">
         1-1) Maximizing the compatibility.
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#minimizing-a-distance-loss-function">
         1-2) Minimizing a distance loss function.
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#synthesizing-zero-shot-class-embeddings-given-phi-y">
     Synthesizing zero-shot class embeddings given
     <span class="math notranslate nohighlight">
      \(\phi(y)\)
     </span>
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#available-data-while-training">
     Available data while training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zero-shot-evlauation-scheme">
   Zero-shot evlauation scheme
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-zero-shot-evaluation-setup">
     ‘Generalized’ zero-shot evaluation setup
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#different-approaches-for-zero-shot-learning">
   Different approaches for zero-shot learning
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-1-learning-by-pairwise-ranking-of-compatibility">
     (1) Case 1 : Learning by pairwise ranking of compatibility
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-2-learning-by-maximizing-probability-function">
     (2) Case 2 : Learning by maximizing probability function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-3-autoencoder-approach">
     (3) Case 3 : Autoencoder approach
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-4-generative-approach">
     (4) Case 4 : Generative approach
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="zero-shot-learning-task-formulation">
<h1>Zero-shot learning task formulation<a class="headerlink" href="#zero-shot-learning-task-formulation" title="Permalink to this headline">#</a></h1>
<p>Now, we’ll go over more detailed formulation of zero-shot learning. Basic task formulation of zero-shot learning frames work is as follows.</p>
<section id="problem-definition">
<h2>Problem definition<a class="headerlink" href="#problem-definition" title="Permalink to this headline">#</a></h2>
<p>Given a dataset of input feature vectors <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and their associated labels <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, we first split the class labels into seen and unseen groups (<span class="math notranslate nohighlight">\(\mathcal{Y}^{seen}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}^{unseen}\)</span>). The resulted ‘seen’ split is composed of <span class="math notranslate nohighlight">\(\mathcal{S}^{seen}\equiv\{\left(x_n, y_n\right)\}_{n=1}^{N}\)</span>, where an input <span class="math notranslate nohighlight">\(x_n\)</span> is a feature vector on a <span class="math notranslate nohighlight">\(D\)</span>-dimensional space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (<span class="math notranslate nohighlight">\(x_n \in \mathcal{X} \stackrel{\text{def}}{=}\mathbb{R}^D\)</span>) and <span class="math notranslate nohighlight">\(y_n\)</span> is one of <span class="math notranslate nohighlight">\(C_0\)</span> label classes (  <span class="math notranslate nohighlight">\(y_n \in \mathcal{Y}^{seen} \equiv\left\{1, \ldots, C_0\right\}\)</span>).</p>
<p>The other set is denoted as the <em>unseen</em> split <span class="math notranslate nohighlight">\(\mathcal{S}^{unseen} \equiv\left\{\left(x_n^{\prime}, y_n^{\prime}\right)\right\}_{n=1}^{N^{\prime}}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{x}_n^{\prime}\)</span> is also a vector from the feature space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (<span class="math notranslate nohighlight">\(x_n^{\prime} \in \mathcal{X}\)</span>), while <span class="math notranslate nohighlight">\(y_n^{\prime}\)</span> is from the other set of classes (<span class="math notranslate nohighlight">\(y_n^{\prime} \in \mathcal{Y}^{unseen} \equiv\)</span> <span class="math notranslate nohighlight">\(\left\{C_0+1, \ldots, C_0+C_1\right\}\)</span>). Note that <span class="math notranslate nohighlight">\(\mathcal{Y}^{seen} \cap \mathcal{Y}^{unseen}=\varnothing\)</span>.</p>
<p>To simulate a zero-shot condition where the model is supposed to infer an input to a novel class,  only a subset of input and ‘seen’ label pairs (<span class="math notranslate nohighlight">\(\mathcal{S}^{seen}\)</span>) are used in training.
At test time, the other subset of the input and ‘unseen’ label pairs (<span class="math notranslate nohighlight">\(\mathcal{S}^{unseen}\)</span>) are used for evaluation.</p>
<p>Another main ingredient for the zero-shot model is the side information, which is often given as an additional representational space of the label classes, <span class="math notranslate nohighlight">\(\{\phi(y) ; y \in \mathcal{Y}^{seen} \cup \mathcal{Y}^{unseen}\}\)</span>, where
<span class="math notranslate nohighlight">\(\phi(y) \in \Phi \equiv \mathbb{R}^{D^{\prime}}\)</span>.</p>
<p>The goal of zero-shot learning is to learn a classfier <span class="math notranslate nohighlight">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span>  that is well-generalized to <span class="math notranslate nohighlight">\(\mathcal{Y}^{unseen}\)</span> even without seeing any training instances for <span class="math notranslate nohighlight">\(\mathcal{Y}^{unseen}\)</span> (<span class="math notranslate nohighlight">\(\mathcal{Y}^{seen} \subset \mathcal{Y}, \mathcal{Y}^{unseen} \subset \mathcal{Y}\)</span>),.</p>
<p>To summarize, given <span class="math notranslate nohighlight">\(\mathcal{S^{seen}}=\left\{\left(x_n, y_n\right), n=1 \ldots N\right\}\)</span>, with <span class="math notranslate nohighlight">\(x_n \in \mathcal{X}^{seen}\)</span> and <span class="math notranslate nohighlight">\(y_n \in \mathcal{Y}^{seen}\)</span>, where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{S}^{seen}\)</span> refers to the set of seen input vectors and their associated classes.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{Y^{seen}}\)</span> is the set of seen classes.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X^{seen}}\)</span> is the set of input vectors that are paired with the seen classes.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(x_n\)</span> is a <span class="math notranslate nohighlight">\(D\)</span>-dimensional input vector in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (<span class="math notranslate nohighlight">\(x_n \in \mathcal{X} \stackrel{\text{def}}{=}\mathbb{R}^D\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(y_n \in \{1,...,C_0\}\)</span> is the class label that corresponds to <span class="math notranslate nohighlight">\(x_n\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the size of the seen training pairs.</p></li>
</ul>
<p>we learn <span class="math notranslate nohighlight">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span> by minimizing the regularized loss function :</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{N} \sum_{n=1}^N L\left(y_n, f\left(x_n ; W\right)\right)+\Omega(W)
\]</div>
<p>, where <span class="math notranslate nohighlight">\(L()\)</span> is a loss function and <span class="math notranslate nohighlight">\(\Omega()\)</span> is a regularization term.</p>
<p>This looks quite similar to a classical supervised learning process, however, the model should be able to make predictions for the general class set <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. At the test phase, the model makes prediction on inputs that are associated with the unseen label set <span class="math notranslate nohighlight">\(\mathcal{Y^{unseen}}\)</span> by calculating the maximum compatibility.</p>
</section>
<section id="modeling-f">
<h2>Modeling <span class="math notranslate nohighlight">\(f\)</span><a class="headerlink" href="#modeling-f" title="Permalink to this headline">#</a></h2>
<section id="compatibility-between-the-input-data-and-the-label-data">
<h3>Compatibility between the input data and the label data.<a class="headerlink" href="#compatibility-between-the-input-data-and-the-label-data" title="Permalink to this headline">#</a></h3>
<p><span class="math notranslate nohighlight">\(f\)</span> is usually modeled by using a certain compatibility function :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(x ; W)=\underset{y \in \mathcal{Y}}{\operatorname{argmax}}F(x, y ; W)\)</span>, where <span class="math notranslate nohighlight">\(F(x, y ; W)\)</span> is a compatibility function that measures how compatible the input is with a class label.</p></li>
</ul>
<p>Since inputs and labels are represented as vectors <span class="math notranslate nohighlight">\(\theta(x), \phi(y)\)</span> using corresponding embedding functions,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is a representational embedding function for input features.</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> is a representational embedding function for class labels as described abolve.</p></li>
</ul>
<p>taking the <span class="math notranslate nohighlight">\(\underset{y \in \mathcal{Y}}{\operatorname{argmax}}\)</span> of compatibility is often acheived by choosing the nearest neighbor vector on the embedding space.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F(x, y ; W)\)</span> can be rewritten as <span class="math notranslate nohighlight">\(F^{\prime}(\theta(x), \phi(y) ; W)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W\)</span> is a learnable matrix (our model).</p></li>
</ul>
<p>And when dealing with explicit attribute annotations for each class, <span class="math notranslate nohighlight">\(f\)</span> can also be modeled in a more explicit fashion. Given explicit attribute classes <span class="math notranslate nohighlight">\(a \in A\)</span>, where <span class="math notranslate nohighlight">\(A\equiv \{1, \ldots, M\}\)</span>, <span class="math notranslate nohighlight">\(f\)</span> can be modeled using the combination of the conditional probabilities of attributes given the input feature.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(x)=\underset{y \in \mathcal{Y}}{\operatorname{argmax}} \prod_{m=1}^M \frac{p\left(a_m^y \mid x\right)}{p\left(a_m^y\right)}\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(M\)</span> : number of attributes</p></li>
<li><p><span class="math notranslate nohighlight">\(a_m^y\)</span> is the m-th attribute of class <span class="math notranslate nohighlight">\(y\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p\left(a_m^y \mid x\right)\)</span> is the attribute probability given input <span class="math notranslate nohighlight">\(x\)</span> which is obtained from the attribute classifiers (our estimator).</p></li>
<li><p><span class="math notranslate nohighlight">\(p\left(a_m^y\right)\)</span> is the attribute prior estimated by the empirical mean of attributes over training classes.</p></li>
</ul>
</li>
<li><p>e.g. Direct Attribute Projection (DAP) and Indirect Attribute Projection (IAP) <span id="id1">[<a class="reference internal" href="../bibliography.html#id49" title="Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Computer Vision and Pattern Recognition (CVPR). 2009.">21</a>]</span></p></li>
</ul>
<a class="reference internal image-reference" href="../_images/DAP.png"><img alt="../_images/DAP.png" src="../_images/DAP.png" style="width: 800px;" /></a>
<p>Training objectives for the compatibility function is as follows.</p>
<section id="maximizing-the-compatibility">
<h4>1-1) Maximizing the compatibility.<a class="headerlink" href="#maximizing-the-compatibility" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>e.g. Linear compatibility function (learnable)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(F(x, y ; W)=\theta(x)^T W \phi(y)\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is a representational embedding function for input features.</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> is a representational embedding function for class labels as described abolve.</p></li>
</ul>
</li>
<li><p>This can also be seen as learning a projection matrix that maximizes the dot product.</p></li>
</ul>
</li>
</ul>
<p>or by</p>
</section>
<section id="minimizing-a-distance-loss-function">
<h4>1-2) Minimizing a distance loss function.<a class="headerlink" href="#minimizing-a-distance-loss-function" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Nonlinear mapping function (neural network layer <span class="math notranslate nohighlight">\(W_1\)</span> and <span class="math notranslate nohighlight">\(W_2\)</span>) trained with a loss function</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\sum_{y \in \mathcal{Y}^{seen}} \sum_{x \in \mathcal{X}_y} \| \phi(y)-W_1 \tanh \left(W_2 \cdot \theta(x)\right) \|^2\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is a representational embedding function for input features.</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> is a representational embedding function for class labels as described abolve.</p></li>
</ul>
</li>
<li><p>Other distance metrics such as cosine distance can also be used.</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>
<section id="synthesizing-zero-shot-class-embeddings-given-phi-y">
<h2>Synthesizing zero-shot class embeddings given <span class="math notranslate nohighlight">\(\phi(y)\)</span><a class="headerlink" href="#synthesizing-zero-shot-class-embeddings-given-phi-y" title="Permalink to this headline">#</a></h2>
<p>Other than directly modeling the relationship between seen and unseen class embeddings, there is another direction of leveraging generative models, such as GAN. Unlike conventional GAN models that generate audio or images directly, zero-shot related GAN models learn to generate feature embeddings given <span class="math notranslate nohighlight">\(\phi(y)\)</span> (side information) as conditional input.</p>
<p>After the generator and discriminator are trained, for any given unseen class <span class="math notranslate nohighlight">\(y\)</span>, unseen class embeddings can be generated by computing <span class="math notranslate nohighlight">\(G(z,\phi(y^{unseen}))\)</span>.</p>
<p>Then a synthetic class embedding <span class="math notranslate nohighlight">\(\{(\tilde{x},y^{unseen},c(y^{unseen}))\}\)</span> can be constructed for training of unseen classes (any arbitrary input features <span class="math notranslate nohighlight">\(\tilde{x}\)</span> can be synthesized). The problem now becomes a simple classification task where .</p>
</section>
<section id="available-data-while-training">
<h2>Available data while training<a class="headerlink" href="#available-data-while-training" title="Permalink to this headline">#</a></h2>
<p>To simulate a proper zero-shot learning situation, unseen classes should be strictly blinded during training phase.
However, depending on the scope of information that the zero-shot model sees during training, there are two broad types of setup. One is inductive zero-shot learning and the other is transductive zero-shot learning.
In transductive learning setup, in addition to the seen classes and their labeled data samples, the model takes unlabeled data samples from the unseen classes into account. This alleviates the projection domain shift problem by letting the model catch the distribution of unseen class instances and learn a more discriminative projection.</p>
<ul class="simple">
<li><p>Inductive zero-shot learning</p>
<ul>
<li><p>A common setup is the inductive zero-shot learning. In this approaches, only labeled training samples and auxiliary information of seen classes are available during training.</p></li>
</ul>
</li>
<li><p>Transductive zero-shot learning</p>
<ul>
<li><p>Labeled training samples, unlabelled test samples, and auxiliary information of all classes are available during training.</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../_images/inductive_transductive.png"><img alt="../_images/inductive_transductive.png" src="../_images/inductive_transductive.png" style="width: 1000px;" /></a>
<!-- A schematic diagram of ZSL versus GZSL. Assume that the seen class contains samples of Otter and Tiger, while the unseen class contains samples of Polar bear and Zebra. (a) During the training phase, both GZSL and ZSL methods have access to the samples and semantic representations of the seen class. (b) During the test phase, ZSL can only recognize samples from the unseen class, while (c) GZSL is able to recognize samples from both seen and unseen classes. -->
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="zero-shot-evlauation-scheme">
<h1>Zero-shot evlauation scheme<a class="headerlink" href="#zero-shot-evlauation-scheme" title="Permalink to this headline">#</a></h1>
<section id="generalized-zero-shot-evaluation-setup">
<h2>‘Generalized’ zero-shot evaluation setup<a class="headerlink" href="#generalized-zero-shot-evaluation-setup" title="Permalink to this headline">#</a></h2>
<p>In conventional zero-shot learning setup, the trained model was evaluated on the set of unseen classes and their associated data samples. Under this formulation,conventional zero-shot learning research have verified that the basic concept of zero-shot knowledge transfer actually works.</p>
<p>However, in the real world problem, the practical advantage of zero-shot learning is in its generalizability where the prediction scope can expand to a large number of classes present on the side information space. <span id="id2">[<a class="reference internal" href="../bibliography.html#id47" title="Abhijit Bendale and Terrance E. Boult. Towards open set deep networks. In Computer Vision and Pattern Recognition (CVPR). 2016.">22</a>]</span> To strictly verify this cabability, the ‘generalized’ zero-shot evaluation had been proposed. Since zero-shot learning models are prone to overfit on the seen classes, they often perform poorly under the generalized zero-shot learning setup.</p>
<p>Since then, generalized zero-shot evaluation became the standard criterion of zero-shot model performance.</p>
<a class="reference internal image-reference" href="../_images/zsl_vs_gzsl.png"><img alt="../_images/zsl_vs_gzsl.png" src="../_images/zsl_vs_gzsl.png" style="width: 800px;" /></a>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="different-approaches-for-zero-shot-learning">
<h1>Different approaches for zero-shot learning<a class="headerlink" href="#different-approaches-for-zero-shot-learning" title="Permalink to this headline">#</a></h1>
<section id="case-1-learning-by-pairwise-ranking-of-compatibility">
<h2>(1) Case 1 : Learning by pairwise ranking of compatibility<a class="headerlink" href="#case-1-learning-by-pairwise-ranking-of-compatibility" title="Permalink to this headline">#</a></h2>
<p>DeViSE: A Deep Visual-Semantic Embedding Model (Frome et al., 2013) <span id="id3">[<a class="reference internal" href="../bibliography.html#id48" title="Andrea Frome, Greg S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc'Aurelio Ranzato, and Tomas Mikolov. Devise: a deep visual-semantic embedding model. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2. 2013.">23</a>]</span></p>
<p>Maximize the following objective function using pairwise ranking:</p>
<div class="math notranslate nohighlight">
\[
\sum_{y \in \mathcal{Y}^{t r}}\left[\Delta\left(y_n, y\right)+F\left(x_n, y ; W\right)-F\left(x_n, y_n ; W\right)\right]_{+}
\]</div>
<ul class="simple">
<li><p>Ranking objective to map training inputs close to continuous embedding vectors corresponding to correct labels.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta\left(y_n, y\right)=1\)</span> if <span class="math notranslate nohighlight">\(y_n=y\)</span>, otherwise 0</p></li>
<li><p>Optimized by gradient descent.</p></li>
</ul>
</section>
<section id="case-2-learning-by-maximizing-probability-function">
<h2>(2) Case 2 : Learning by maximizing probability function<a class="headerlink" href="#case-2-learning-by-maximizing-probability-function" title="Permalink to this headline">#</a></h2>
<p>Learning to detect unseen object classes by between-class attribute transfer (Lampert et al., 2009) <span id="id4">[<a class="reference internal" href="../bibliography.html#id49" title="Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Computer Vision and Pattern Recognition (CVPR). 2009.">21</a>]</span></p>
<p>CONSE (Norouzi et al., 2014) <span id="id5">[<a class="reference internal" href="../bibliography.html#id50" title="Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Gregory S. Corrado, and Jeffrey Dean. Zero-shot learning by convex combination of semantic embeddings. In International Conference on Learning Representations (ICLR). 2013.">24</a>]</span></p>
<p>Instead of learning the mapping function <span class="math notranslate nohighlight">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span> explicitly, learn a classifier from training inputs to seen labels. The probability of an input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> belonging to a class label <span class="math notranslate nohighlight">\(y \in \mathcal{Y}_{seen}\)</span> can then be estimated, denoted <span class="math notranslate nohighlight">\(p_{seen}(y \mid x)\)</span>, where <span class="math notranslate nohighlight">\(\sum_{y=1}^{n} p_{seen}(y \mid x)=1\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(x, t)\)</span> : <span class="math notranslate nohighlight">\(\mathrm{t}^{th}\)</span> most likely label for input <span class="math notranslate nohighlight">\(x\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(f(x, 1) \equiv \underset{y \in \mathcal{Y}_{seen}}{\operatorname{argmax}} p_{seen}(y \mid x)\)</span> : probability of an input <span class="math notranslate nohighlight">\(x\)</span> belonging to a seen class:</p></li>
</ul>
</li>
<li><p>Each class label <span class="math notranslate nohighlight">\(y(1 \leq y \leq n)\)</span> is associated with a semantic embedding vector <span class="math notranslate nohighlight">\(\phi(y) \in \Phi \equiv \mathbb{R}^{D^{\prime}}\)</span>.</p></li>
<li><p>Given a test input, the ConSE simply runs the convolutional classifier and considers the top T predictions of the model. Then, the convex combination of the corresponding <span class="math notranslate nohighlight">\(T\)</span> semantic embedding vectors in the semantic space is computed, which defines a deterministic transformation from the outputs of the Softmax classifier into the embedding space.</p></li>
</ul>
<p>Combination of semantic embeddings <span class="math notranslate nohighlight">\((\phi)\)</span> is used to assign an unknown input to an unseen class:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{Z} \sum_{i=1}^T p_{seen}(f(x, t) \mid x) \phi(f(x, t))
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Z \)</span>: normalization factor given by <span class="math notranslate nohighlight">\(Z=\sum_{i=1}^T p_{seen}(f(x, t) \mid x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(T\)</span> : hyperparameter of controlling the maximum number of semantic embedding vectors to be considered.</p></li>
</ul>
<p>If the classifier is confident in its prediction of a label <span class="math notranslate nohighlight">\(y\)</span> for <span class="math notranslate nohighlight">\(x\)</span>, i.e., <span class="math notranslate nohighlight">\(p_{seen}(y \mid x) \approx 1\)</span>, then <span class="math notranslate nohighlight">\(f(x) \approx \phi(y)\)</span>. If not, predicted semantic embedding is somewhere between <span class="math notranslate nohighlight">\(T\)</span> most likely classes (weighted-sum).</p>
</section>
<section id="case-3-autoencoder-approach">
<h2>(3) Case 3 : Autoencoder approach<a class="headerlink" href="#case-3-autoencoder-approach" title="Permalink to this headline">#</a></h2>
<p>SAE (Kodirov et al., 2017) <span id="id6">[<a class="reference internal" href="../bibliography.html#id51" title="Elyor Kodirov, Tao Xiang, and Shaogang Gong. Semantic autoencoder for zero-shot learning. In CoRR. 2017.">25</a>]</span></p>
<p>Minimize the reconstruction loss (similar to the linear auto-encoder).</p>
<div class="math notranslate nohighlight">
\[
\min _W\left\|\theta(x)-W^T \phi(y)\right\|^2+\lambda\|W \theta(x)-\phi(y)\|^2,
\]</div>
<ul class="simple">
<li><p>Learns a linear projection from <span class="math notranslate nohighlight">\(\theta(x)\)</span> to <span class="math notranslate nohighlight">\(\phi(y)\)</span>, being similar to above approaches.</p></li>
<li><p>Reconstruction of the original input embedding is set as the training objective .</p></li>
</ul>
</section>
<section id="case-4-generative-approach">
<h2>(4) Case 4 : Generative approach<a class="headerlink" href="#case-4-generative-approach" title="Permalink to this headline">#</a></h2>
<p>f-CLSWGAN (Xian et al., 2017) <span id="id7">[<a class="reference internal" href="../bibliography.html#id52" title="Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for zero-shot learning. In CoRR. 2017.">26</a>]</span></p>
<ul class="simple">
<li><p>Phase 1. Using seen class and image pairs, train a conditional GAN architecture to synthesize image feature vectors.</p></li>
<li><p>Phase 2. Use the generator to synthesize pseudo image feature vectors for unseen classes.</p></li>
<li><p>Phase 3. Train a classifier with the synthesized image feature vectors and their associated (unseen) classes</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./foundations-zsl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="01_overview_zsl.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Zero-Shot Learning Foundations</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="03_side_information.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Side Information</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Yu Wang, Hugo Flores García, and Jeong Choi<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>